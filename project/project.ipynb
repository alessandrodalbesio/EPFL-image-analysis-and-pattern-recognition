{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Project\n",
    "\n",
    "\n",
    "**Group ID:** 5\n",
    "\n",
    "**Author 1 (sciper):** Camillo Nicolò De Sabbata (335004)  \n",
    "\n",
    "**Author 2 (sciper):** Gianluca Radi (334736)\n",
    "\n",
    "**Author 3 (sciper):** Alessandro Dalbesio (352298)\n",
    "\n",
    "**Release date:** 27.04.2023\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The assignments are designed to teach practical implementation of the topics presented during class as well as preparation for the final project, which is a practical project which ties together the topics of the course. \n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook !** rerun the notebook from scratch `Kernel` > `Restart & Run All`\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "In this project, you will be working on solving tiling puzzles using image analysis and pattern recognition techniques. Tiling puzzles are a classic type of puzzle game that consists of fitting together pieces of a given shape (in this case squared to form a complete image. The goal of this project is to develop an algorithm that can automatically reconstruct tiling puzzles from a single input image. \n",
    "\n",
    "## 1. Data\n",
    "\n",
    "### Input data\n",
    "To achieve your task, you will be given images that look like this:\n",
    "\n",
    "\n",
    "![train_00.png](data_project/project_description/train_00.png)\n",
    "\n",
    "### Example puzzle content\n",
    "Example of input of solved puzzles. <br>\n",
    "<br>Solution 1<br>\n",
    "<img src=\"data_project/project_description/solution_example.png\" width=\"512\"/>\n",
    "<br>Solution 2<br>\n",
    "<img src=\"data_project/project_description/solution_example2.jpg\" width=\"512\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Image layout\n",
    "\n",
    "- The input for the program will be a single image with a size of __2000x2000 pixels__, containing the pieces of the tiling puzzles randomly placed in it. The puzzles sizes vary from __3x3, 3x4, or 4x4__ size. \n",
    "    -__You are guaranteed to always have the exact number of pieces for each puzzle__ \n",
    "        -For each puzzle you always are expected to find exaclty 9,12,16 pieces\n",
    "        -If you find something else, either you are missing pieces, or added incorrect pieces for the puzzle\n",
    "\n",
    "- The puzzle pieces are square-shaped with dimensions of 128x128 pixels (before rotation). \n",
    "\n",
    "- The input image will contain pieces from __two or three (but never four)__ different tiling puzzles, as well as some __extra pieces (outliers)__ that do not belong to either puzzle.\n",
    "\n",
    "\n",
    "## 2. Tasks (Total 20 points) \n",
    "\n",
    "\n",
    "The project aims to:\n",
    "1) Segment the puzzle pieces from the background (recover the pieces of 128x128 pixels)   \\[ __5 points__ \\] \n",
    "\n",
    "2) Extract features of interest from puzzle pieces images \\[ __5 points__ \\]   \n",
    "\n",
    "3) Cluster puzzle pieces to identify which puzzle they belong, and identify outliers.  \\[ __5 points__ \\]   \n",
    "\n",
    "4) Solve tiling puzzle (find the rotations and translations to correctly allocate the puzzle pieces in a 3x3, 3x4 or 4x4 array.) \\[ __5 points__ \\]   \n",
    "\n",
    "##### The images used for the puzzles have self-repeating patterns or textures, which ensures that all puzzle pieces contain more or less the same features regardless of how they were cut. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2. Output solution pieces.\n",
    "\n",
    "For each inpute image, the output solution will include N images with solved puzzles, where N is the number of puzzles in the input image. and M images, that are Each of these images will contain the solved solution to one of the N puzzles in the input. \n",
    "\n",
    "\n",
    "-  Example input:  train_05.png\n",
    "\n",
    "- Example solution:\n",
    "        -solution_05_00.png solution_05_01.png solution_05_02.png \n",
    "        -outlier_05_00.png outlier_05_01.png outlier_05_02.png ...\n",
    "\n",
    "- Example input:  train_07.png\n",
    "- Example solution:\n",
    "        -solution_07_00.png solution_07_01.png \n",
    "        -outlier_07_00.png outlier_07_01.png outlier_07_02.png ...\n",
    "\n",
    "\n",
    "__Watch out!__ output resolution should always be like this:  \n",
    "<table ><tr><th >Puzzle pieces </th><th> pixel dimentions </th> <th> pixel dimentions </th> <tr>\n",
    "<tr><td> 3x3 </td><td> 384x384 </td><td> 3(128)x3(128)</td> <tr>\n",
    "<tr><td> 3x4 </td><td> 384x512 </td><td> 3(128)x4(128)</td><tr>\n",
    "<tr><td> 4x4 </td><td> 512x512 </td><td> 4(128)x4(128)</td><tr>\n",
    "<tr><td> 1x1 (outlier)</td><td> 128x128 </td><td> (1)128x(1)128 </td><tr><table>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Order of the solutions (and rotations) it's not a problem for the grading__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the output solution will be a final image of resolution (1283)x(1283), with each piece correctly placed in its corresponding location in the 3x3 array. Similarly, if the puzzle consists of 3x4 or 4x4 pieces, the output solution will be an image of resolution (1283)x(1284) or (1284)x(1284)\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Data folder Structure\n",
    "\n",
    "You can download the data for the project here: [download data](https://drive.google.com/drive/folders/1k3xTH0ZhpqZb3xcZ6wsOSjLzxBNYabg3?usp=share_link)\n",
    "\n",
    "```\n",
    "data_project\n",
    "│\n",
    "└─── project_description\n",
    "│    │    example_input.png      # example input images\n",
    "│    │    example_textures1.png      # example input images\n",
    "│    │    example_textures2.png      # example input images\n",
    "│    └─── ultimate_test.jpg   # If it works on that image, you would probably end up with a good score\n",
    "│\n",
    "└─── train\n",
    "│    │    train_00.png        # Train image 00\n",
    "│    │    ...\n",
    "│    │    train_16.png        # Train image 16\n",
    "│    └─── train_labels.csv    # Ground truth of the train set\n",
    "|    \n",
    "└────train_solution\n",
    "│    │    solution_00_00.png        # Solution puzzle 1 from Train image 00\n",
    "│    │    solution_00_01.png        # Solution puzzle 2 from Train image 00\n",
    "│    │    solution_00_02.png        # Solution Puzzle 3 from Train image 00\n",
    "│    │    outlier_00_00.png         # outlier     from Train image 00\n",
    "│    │    outlier_00_01.png         # outlier     from Train image 00\n",
    "│    │    outlier_00_03.png         # outlier     from Train image 00\n",
    "│    │    ...\n",
    "│    │    solution_15_00.png        # Solution puzzle 1 from Train image 15\n",
    "│    │    solution_15_01.png        # Solution puzzle 2 from Train image 15\n",
    "│    │    outlier_15_00.png         # outlier     from Train image 15\n",
    "│    └─── outlier_15_01.png         # outlier     from Train image 15\n",
    "│\n",
    "└─── test\n",
    "     │    test_00.png         # Test image 00 (day of the exam only)\n",
    "     │    ...\n",
    "     └─── test_xx.png             # Test image xx (day of the exam only)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "**Before the exam**\n",
    "   - Create a zipped folder named **groupid_xx.zip** that you upload on moodle (xx being your group number).\n",
    "   - Include a **runnable** code (Jupyter Notebook and external files) and your presentation in the zip folder.\n",
    "   \n",
    "**The day of the exam**\n",
    "   - You will be given a **new folder** (test folder) with few images, but **no ground truth** (no solutions).\n",
    "   - We will ask you to run your pipeline in **real time** and to send us your prediction of the task you obtain with the provided function **save_results**. \n",
    "   - On our side, we will compute the performance of your classification algorithm. \n",
    "   - To evaluate your method, we will use the **evaluate_solution** function presented below. To understand how the provided functions work, please read the documentation of the functions in **utils.py**.\n",
    "   - **Please make sure your function returns the proper data format to avoid points penalty on the day of the exam**. \n",
    "---\n",
    "\n",
    "\n",
    "## 4. Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load images\n",
    "import os \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import pandas as pd\n",
    "from skimage.morphology import closing\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "\n",
    "GROUP_ID = 5\n",
    "DEFAULT_SAVE_FOLDER = \"data_project/results\"\n",
    "DEFAULT_INPUT_FOLDER = \"data_project/train\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main variables definition\n",
    "PIECES_SIZE = 128 # Final size of the pieces\n",
    "TOO_SMALL_AREA = 125 * 125 # Remove pieces that are too small (and so that are most likely not pieces)\n",
    "ENLARGE_AREA = 5 # Needed to enlarge the reduced area based on the contourns to be able to have with precision also the angles of the pieces\n",
    "ENLARGE_BOX_AREA = 5 # Needed to enlarge the area of the boxes to be able to have better performances\n",
    "\n",
    "\n",
    "# Definition of plotting images functions\n",
    "def plot_first_stage_results(original_image: np.ndarray, canny_image: np.ndarray, flood_fill_image: np.ndarray, original_image_with_contourn: np.ndarray):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title(\"Original image\")\n",
    "    axs[1].imshow(canny_image)\n",
    "    axs[1].set_title(\"Canny image\")\n",
    "    axs[2].imshow(flood_fill_image)\n",
    "    axs[2].set_title(\"Flood fill image\")\n",
    "    axs[3].imshow(original_image_with_contourn)\n",
    "    axs[3].set_title(\"Original image with contourn\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_second_stage_results(image_cropped: np.ndarray, image_canny_cropped: np.ndarray, image_canny_cropped_rotated: np.ndarray, image_canny_cropped_rotated_cleaned: np.ndarray, final_image: np.ndarray):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    # Image cropped\n",
    "    axs[0].imshow(image_cropped)\n",
    "    axs[0].set_title(\"Image cropped\")\n",
    "    \n",
    "    # Canny filter image cropped gray\n",
    "    axs[1].imshow(image_canny_cropped, cmap='gray')\n",
    "    axs[1].set_title(\"Canny image cropped\")\n",
    "    \n",
    "    # Canny filter image cropped rotated gray\n",
    "    axs[2].imshow(image_canny_cropped_rotated, cmap='gray')\n",
    "    axs[2].set_title(\"Area reduction based on the box\")\n",
    "    \n",
    "    # Canny filter image cropped rotated gray cleaned\n",
    "    axs[3].imshow(image_canny_cropped_rotated_cleaned)\n",
    "    axs[3].set_title(\"Area reduction based on the contourn\")\n",
    "    \n",
    "    # Final image\n",
    "    axs[4].imshow(final_image)\n",
    "    axs[4].set_title(\"Final image\")\n",
    "    \n",
    "    # Show the images\n",
    "    plt.show()\n",
    "\n",
    "# Resize the pieces\n",
    "def resize_pieces(image):\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Resize the image if it's too big\n",
    "    if h > PIECES_SIZE:\n",
    "        pixels_to_crop = h - PIECES_SIZE\n",
    "        top = pixels_to_crop // 2\n",
    "        bottom = pixels_to_crop - top\n",
    "        image = image[top:-bottom, :]\n",
    "    if w > PIECES_SIZE:\n",
    "        pixels_to_crop = w - PIECES_SIZE\n",
    "        right = pixels_to_crop // 2\n",
    "        left = pixels_to_crop - right\n",
    "        image = image[:, left:-right]\n",
    "\n",
    "    # Resize the image if it's too small\n",
    "    if w < PIECES_SIZE or h < PIECES_SIZE:\n",
    "        image = cv2.resize(image, (PIECES_SIZE, PIECES_SIZE))\n",
    "\n",
    "    # Return the image\n",
    "    return image\n",
    "\n",
    "# Rotate the image\n",
    "def rotate_piece(image, angle_rotation):\n",
    "    rows, cols = image.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle_rotation, 1)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    return image    \n",
    "\n",
    "# Get the background of the image\n",
    "def bc_image(image):\n",
    "    # Copy the image\n",
    "    img_copy = image.copy()\n",
    "\n",
    "    # Perform a flood fill\n",
    "    h, w = image.shape[:2]\n",
    "    mask_fill = np.zeros((h + 2, w + 2), np.uint8)\n",
    "\n",
    "    # Define the starting points criteria\n",
    "    starting_points = [(0, 0), (0, 1999), (1999, 0), (1999, 1999)]\n",
    "    for point in starting_points:\n",
    "        cv2.floodFill(img_copy, mask_fill, point, 0)\n",
    "    \n",
    "    # Return the background\n",
    "    return mask_fill[0:-2, 0:-2]\n",
    "\n",
    "def crop_coordinated(refImage, refPixelValue):\n",
    "    h,w = refImage.shape[:2]\n",
    "    top, left, bottom, right = 0, 0, 0, 0\n",
    "    for i in range(h):\n",
    "        if np.any(refImage[i, :] != 0):\n",
    "            top = i\n",
    "            break\n",
    "        \n",
    "    for i in range(h-1, 0, -1):\n",
    "        if np.any(refImage[i, :] != 0):\n",
    "            bottom = i\n",
    "            break\n",
    "        \n",
    "    for i in range(w):\n",
    "        if np.any(refImage[:, i] != 0):\n",
    "            left = i\n",
    "            break\n",
    "\n",
    "    for i in range(w-1, 0, -1):\n",
    "        if np.any(refImage[:, i] != 0):\n",
    "            right = i\n",
    "            break   \n",
    "\n",
    "    return top+1, left+1, bottom-1, right-1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation with contours \n",
    "def segmentation(image: np.ndarray, plot_for_presentation: bool = False) -> list[np.ndarray]:\n",
    "    ### First part: identification of the approximate position of the pieces in the big image ###\n",
    "    \n",
    "    # Get the image size\n",
    "    h_image, w_image = image.shape[:2]\n",
    "    \n",
    "    # Apply Canny to the image\n",
    "    img_canny = cv2.Canny(image, 25, 50)\n",
    "\n",
    "    # Apply first a closing to the image to remove the small holes\n",
    "    kernel = np.ones((11, 11), np.uint8)\n",
    "    img_canny = closing(img_canny, kernel)\n",
    "    \n",
    "    # Apply a very soft dilation to the image to connect the pieces\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    img_canny = cv2.dilate(img_canny, kernel, iterations=1)\n",
    "\n",
    "    # Get the background isolated\n",
    "    bc = bc_image(img_canny)\n",
    "\n",
    "    # Find contours on the bc image\n",
    "    contours, hierarchy = cv2.findContours(bc, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)            \n",
    "    contours = [c for c, h in zip(contours, hierarchy[0]) if h[3] != -1]\n",
    "\n",
    "    # Plot the results\n",
    "    if plot_for_presentation:\n",
    "        image_with_contours = image.copy()\n",
    "        cv2.drawContours(image_with_contours, contours, -1, (255, 255, 255), 5)\n",
    "        plot_first_stage_results(image, img_canny, bc, image_with_contours)\n",
    "\n",
    "\n",
    "    ### Second part: extraction of the pieces of the puzzles, rotation and cropping ###    \n",
    "    \n",
    "    images = [] # Pieces to return\n",
    "    for contour in contours:\n",
    "        # Get the bounding box\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Filter out the pieces that are too small (Is due to noise produced in the previous steps)\n",
    "        if w * h < TOO_SMALL_AREA: continue\n",
    "\n",
    "        # Compute the angle of rotation\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        angle = rect[2]\n",
    "\n",
    "        # Draw the contour on an empty image\n",
    "        box = np.intp(cv2.boxPoints(rect))\n",
    "        image_box = np.zeros((h_image, w_image), np.uint8)\n",
    "        cv2.drawContours(image_box, [box], 0, 255, 5)\n",
    "\n",
    "        # Crop the images based on the bounding box (in both cases the area is slightly enlarged)\n",
    "        image_cropped = image[max(y-ENLARGE_AREA,0):min(y+h+ENLARGE_AREA, h_image), max(x-ENLARGE_AREA,0):min(x+w+ENLARGE_AREA, w_image)]\n",
    "        image_box_cropped = image_box[max(y-ENLARGE_AREA,0):min(y+h+ENLARGE_AREA, h_image), max(x-ENLARGE_AREA,0):min(x+w+ENLARGE_AREA, w_image)]\n",
    "\n",
    "        # Convert the image into grayscale and apply a canny filter\n",
    "        image_cropped_canny_gray = cv2.Canny(cv2.cvtColor(image_cropped, cv2.COLOR_BGR2GRAY), 25, 50)\n",
    "\n",
    "        # Rotate both the images\n",
    "        image_cropped_rotated = rotate_piece(image_cropped, angle)\n",
    "        image_cropped_canny_gray_rotated = rotate_piece(image_cropped_canny_gray, angle)\n",
    "        image_box_cropped_rotated = rotate_piece(image_box_cropped, angle)\n",
    "\n",
    "        # Crop the image_cropped_canny_gray_rotated based on image_box_cropped_rotated\n",
    "        top, left, bottom, right = crop_coordinated(image_box_cropped_rotated, 255)\n",
    "        image_cropped_canny_gray_rotated[0:top, :] = 0\n",
    "        image_cropped_canny_gray_rotated[:, 0:left] = 0\n",
    "        image_cropped_canny_gray_rotated[bottom:, :] = 0\n",
    "        image_cropped_canny_gray_rotated[:, right:] = 0\n",
    "\n",
    "        # Get the final image\n",
    "        top_f, left_f, bottom_f, right_f = crop_coordinated(image_cropped_canny_gray_rotated, 255)\n",
    "        final_image = image_cropped_rotated[top_f:bottom_f, left_f:right_f]\n",
    "\n",
    "        # Append the image to the list\n",
    "        images.append(final_image)\n",
    "\n",
    "        if plot_for_presentation:\n",
    "            # Draw on image_cropped_canny_gray_rotated the top, left, bottom and right\n",
    "            img_plot_1 = image_cropped_canny_gray_rotated.copy()\n",
    "            img_plot_1[top, :] = 255\n",
    "            img_plot_1[bottom, :] = 255\n",
    "            img_plot_1[:, left] = 255\n",
    "            img_plot_1[:, right] = 255\n",
    "\n",
    "            img_plot_2 = image_cropped_canny_gray_rotated.copy()\n",
    "            img_plot_2[top_f, :] = 255\n",
    "            img_plot_2[bottom_f, :] = 255\n",
    "            img_plot_2[:, left_f] = 255\n",
    "            img_plot_2[:, right_f] = 255            \n",
    "\n",
    "            plot_second_stage_results(image_cropped, image_cropped_canny_gray, img_plot_1, img_plot_2, final_image)\n",
    "    \n",
    "    images = [cv2.resize(image, (128, 128)) for image in images]\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Description\n",
    "    ----------\n",
    "    This class is used to extract the features from the images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Description\n",
    "        ----------\n",
    "        This function initializes the FeatureExtractor class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load the model\n",
    "        self.model = vgg16(pretrained=True)\n",
    "        # Remove the last layer of the model\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        # Freeze the parameters of the model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Description\n",
    "        ----------\n",
    "        This function is used to extract the features from the images\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            - x (torch.Tensor): Image to extract the features from\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "            - torch.Tensor: Features of the image\n",
    "        \"\"\"\n",
    "        # Pass the image through the model\n",
    "        # image shape: (3, 128, 128)\n",
    "        # x shape: (1, 2048, 1, 1)\n",
    "        x = self.model(image)\n",
    "        # features shape: 2048\n",
    "        features = torch.flatten(x).detach().numpy()\n",
    "        # Return the features\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_edges(image):\n",
    "    '''Returns a version of the image with ehanced edges'''\n",
    "    # Convert the image into grayscale\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply a gaussian blur to the image\n",
    "    image_gray = cv2.GaussianBlur(image_gray, (5, 5), 0)\n",
    "    # Apply a canny filter\n",
    "    image_canny = cv2.Canny(image_gray, 25, 50)\n",
    "    ## Apply first a closing to the image to remove the small holes\n",
    "    image_canny = closing(image_canny)\n",
    "    # Apply the mask to enhance the edges\n",
    "    image_canny = cv2.bitwise_and(image, image, mask=image_canny)\n",
    "    return image_canny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    # Convert the image to a tensor\n",
    "    # if the image is bigger, resize it to (128, 128, 3)\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    features = []\n",
    "    for image in images:\n",
    "        enhanced = torch.tensor(enhance_edges(image)).float()\n",
    "        image = torch.tensor(image).float()\n",
    "        # image shape: (1, 3, 128, 128)\n",
    "        image = image.permute(2, 0, 1).unsqueeze(0)\n",
    "        enhanced = enhanced.permute(2, 0, 1).unsqueeze(0)\n",
    "        # Extract the features from the image\n",
    "        image_features = feature_extractor(image)\n",
    "        enhance_features = feature_extractor(enhanced)\n",
    "        features.append(np.concatenate((image_features, enhance_features)))\n",
    "    # Return the features\n",
    "    pca = PCA(n_components=6)\n",
    "    image_features = pca.fit_transform(features)\n",
    "    return image_features\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a number, \n",
    "def separate_number(number: int) -> list[int]:\n",
    "    \"\"\"Description\n",
    "    ----------\n",
    "    Args:\n",
    "    ----------\n",
    "        - number (int): Number to be separated\n",
    "    Returns:\n",
    "    ----------\n",
    "        - list[dict[int]]: List of dictionaries that contain the number of pieces of each type\n",
    "    \"\"\"\n",
    "    pieces = []\n",
    "    # Divide the number in 9, 12 and 16\n",
    "    for i in range(0, number//9 + 1):\n",
    "        for j in range(0, number//12 + 1):\n",
    "            for k in range(0, number//16 + 1):\n",
    "                # Verify if the sum of the pieces is equal to the number\n",
    "                if 0 < number - 9*i - 12*j - 16*k <= 3:\n",
    "                    pieces.append({\"9\": i, \"12\": j, \"16\": k, \"rest\": number - 9*i - 12*j - 16*k})\n",
    "    # Return the list of pieces\n",
    "    return pieces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(sizes: list[int], image_features: np.ndarray) -> list[np.ndarray]:\n",
    "    \"\"\"Description\n",
    "    ----------\n",
    "    This function is used to cluster the images in the list\n",
    "    Args:\n",
    "    ----------\n",
    "        - sizes (list[int]): This list contains the number of pieces of each type\n",
    "        - image_features (list[np.ndarray]): This list contains the features of each image\n",
    "    Returns:\n",
    "    ----------\n",
    "        - list[np.ndarray]: labels of the clusters\n",
    "    \"\"\"\n",
    "    # Implement K Means considering that the sizes of each cluster are given\n",
    "    # Create a list of labels\n",
    "    possible_labels = range(len(sizes))\n",
    "    k_means_iterations = 5000\n",
    "    k_means_steps = 10\n",
    "    # Randomly initialize the labels according to the sizes\n",
    "    best_labels = [possible_labels[i] for i in range(len(sizes)) for _ in range(sizes[i])]\n",
    "    best_inertia = float(\"inf\")\n",
    "    min_value = np.min(image_features)\n",
    "    max_value = np.max(image_features)\n",
    "    # Iterate over the number of iterations\n",
    "    for iteration in range(k_means_iterations):\n",
    "        if best_labels != [] and iteration > 500:\n",
    "            break\n",
    "        # Assign random centroids (within the range of the features)\n",
    "        centroids = np.random.uniform(low=min_value, high=max_value, size=(len(sizes), image_features.shape[1]))\n",
    "        labels = []\n",
    "        previous_centroids = centroids\n",
    "        # Iterate over the number of steps\n",
    "        for _ in range(k_means_steps):\n",
    "            previous_centroids = centroids\n",
    "            # Assign each feature to the closest centroid (cityblock distance)\n",
    "            labels = np.array([np.argmin([np.linalg.norm(feature - centroid, ord=1) for centroid in centroids]) for feature in image_features])\n",
    "            # Update the centroids\n",
    "            centroids = [np.mean(image_features[labels == i], axis=0) for i in possible_labels]\n",
    "            # If and centroids are the same as the previous ones, break the loop\n",
    "            if np.allclose(previous_centroids, centroids, rtol=0, atol=1e-03):\n",
    "                break\n",
    "        # Calculate the inertia\n",
    "        inertia = np.mean([np.linalg.norm(feature - centroids[labels[i]]) for i, feature in enumerate(image_features)])\n",
    "        # If the inertia is better than the best one, save the labels and the inertia and the cluster sizes respect the original sizes\n",
    "        if inertia < best_inertia and sorted(Counter(labels).values()) == sorted(sizes):\n",
    "            best_inertia = inertia\n",
    "            best_labels = labels\n",
    "    # Return the labels\n",
    "    return best_labels, best_inertia\n",
    "\n",
    "def plot_clusters(clusters):\n",
    "    for cluster in clusters.values():\n",
    "        fig, ax = plt.subplots(1, len(cluster), figsize=(len(cluster)*5, 5))\n",
    "        if len(cluster) == 1:\n",
    "            ax.imshow(cluster[0])\n",
    "        else:\n",
    "            for i, image in enumerate(cluster):\n",
    "                ax[i].imshow(image)\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "def clustering(image_features: list[np.ndarray], images: list[np.ndarray], verbose: False) -> dict[int, list[np.ndarray]]:\n",
    "    '''clustering with K-means with predefined number of samples in each cluster'''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    cluster_sizes_possibilities = []\n",
    "    for separation in separate_number(len(image_features)):\n",
    "        cluster_sizes_possibilities.append(separation[\"9\"] * [9] + separation[\"12\"] * [12] + separation[\"16\"] * [16] + [separation[\"rest\"]] )\n",
    "    possible_labels = []\n",
    "    intertias = []\n",
    "    for sizes in cluster_sizes_possibilities:\n",
    "        labels, intertia = kmeans(sizes, image_features)\n",
    "        possible_labels.append(labels)\n",
    "        intertias.append(intertia)\n",
    "    # Select the best labels\n",
    "    best_labels = possible_labels[np.argmin(intertias)]\n",
    "    num_clusters = max(best_labels)+1\n",
    "    clusters = {i: [] for i in range(num_clusters)}\n",
    "    for i in range(len(best_labels)):\n",
    "        clusters[best_labels[i]].append(images[i])\n",
    "    if verbose:\n",
    "        plot_clusters(clusters)\n",
    "    return clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_image(image_index: int ,  folder: str = DEFAULT_SAVE_FOLDER) -> np.array:\n",
    "    \"\"\"Description\n",
    "    ----------\n",
    "    Function that loads an image from a folder and returns it as a numpy array\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "        - image_index (int): index of the image to load (train_XX.png)\n",
    "        - folder (str, optional): name of the folder where the image is. Defaults to \"data_project/train\".\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        - im (np.array): image as a numpy array of dimension 2000 x 2000 (RGB format)\n",
    "    \"\"\"\n",
    "    filename = \"train_{}.png\".format(str(image_index).zfill(2))   \n",
    "    im= Image.open(os.path.join(folder,filename)).convert('RGB')\n",
    "    im = np.array(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_image(pieces: list[np.ndarray]) -> np.ndarray:\n",
    "    ''' Compose the image from the pieces '''\n",
    "    # The pieces can only be 9, 12 or 16, and the images can be 3x3, 3x4 or 4x4\n",
    "    # each piece is 128x128\n",
    "    num_pieces = len(pieces)\n",
    "    size_map = {9: (3, 3), 12: (3, 4), 16: (4, 4)}\n",
    "    rows, cols = size_map[num_pieces]\n",
    "    empty_image = np.zeros((rows*128, cols*128, 3))\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            empty_image[i*128:(i+1)*128, j*128:(j+1)*128, :] = pieces[i*cols+j]\n",
    "    return (empty_image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solution_puzzles(image_index: int, solved_puzzles: list[np.array], outliers: list[np.array], folder: str = DEFAULT_SAVE_FOLDER) -> None:\n",
    "    \"\"\"Description:\n",
    "    ----------\n",
    "    Function that saves the various puzzles and outliers in the folder data_project/train_solution_XX (where XX is the group number) in the following format:\n",
    "        - solution_XX_YY.png (where XX is the image index and YY is the puzzle index)\n",
    "        - outlier_XX_YY.png (where XX is the image index and YY is the outlier index)\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "        - image_index (int): index of the image to save (train_XX.png)\n",
    "        - solved_puzzles (list[np.array]): list containing the saved puzzles (each element of the list rappresents a solved puzzle)\n",
    "        - outliers (list[np.array]): list containing the outliers (each outlier is a numpy array 128 x 128 x 3)\n",
    "        - folder (str, optional): Initial name of the folder in which the solutions will be saved (folder'_solution_5') . Default to \"data_project/train\" (solutions will be saved in data_project/train_solution_5)\n",
    "    \"\"\"\n",
    "    path_solution = os.path.join(folder + \"_solution_{}\".format(str(GROUP_ID).zfill(2)))\n",
    "    if not os.path.isdir(path_solution):\n",
    "        os.mkdir(path_solution)\n",
    "\n",
    "    for i, puzzle in enumerate(solved_puzzles):\n",
    "        filename = os.path.join(path_solution, \"solution_{}_{}.png\".format(str(image_index).zfill(2), str(i).zfill(2)))\n",
    "        Image.fromarray(puzzle).save(filename)\n",
    "\n",
    "    for i , outlier in enumerate(outliers):\n",
    "        filename =os.path.join(path_solution, \"outlier_{}_{}.png\".format(str(image_index).zfill(2), str(i).zfill(2)))\n",
    "        Image.fromarray(outlier).save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_export_puzzles_image(image_index , folder = DEFAULT_INPUT_FOLDER):\n",
    "    \"\"\"Description:\n",
    "    ----------\n",
    "    Wrapper function to load an image with a specified index and save the solutions in the folder designed by the variable folder\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    - image_index (int): index of the image to load (train_XX.png)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    - image_loaded (np.array): image as a numpy array of dimension 2000 x 2000 (RGB format)\n",
    "    - solved_puzzles (list[np.array]): list containing the saved puzzles (each element of the list rappresents a solved puzzle)\n",
    "    - outlier_images (list[np.array]): list containing the outliers (each outlier is a numpy array 128 x 128 x 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the image\n",
    "    image_loaded = load_input_image(image_index, folder = folder)\n",
    "    \n",
    "    # Segment the images \n",
    "    images = segmentation(image_loaded)\n",
    "    features = extract_features(images)\n",
    "    clusters = clustering(features, images, verbose = True)\n",
    "    \n",
    "    # Put in the outlier_images list the cluster with the smallest number of elements\n",
    "    outlier_images = clusters[np.argmin([len(cluster) for cluster in clusters.values()])]\n",
    "    del clusters[np.argmin([len(cluster) for cluster in clusters.values()])]\n",
    "    # Put into solved_puzzles the other clusters\n",
    "    solved_puzzles = [compose_image(cluster) for cluster in clusters.values()]\n",
    "    save_solution_puzzles (image_index , solved_puzzles , outlier_images , folder = folder)\n",
    "    \n",
    "    return image_loaded , solved_puzzles , outlier_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,12):\n",
    "    solve_and_export_puzzles_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
